{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# Read data\n",
    "train_data = pd.read_csv(\"train.csv\", sep = '\\t')\n",
    "test_data = pd.read_csv(\"test.csv\", sep = '\\t')\n",
    "test_label = pd.read_csv(\"sample_submission.csv\", sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "def remove_punctuation(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "def scrub_text(text):\n",
    "    # To lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove html markup\n",
    "    text = re.sub(\"(<.*?>)\", \"\", text)\n",
    "    \n",
    "    # Remove non-ascii and digits\n",
    "    text = re.sub(\"(\\\\W|\\\\d)\", \" \", text)\n",
    "    \n",
    "    # Remove whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = remove_punctuation(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "train_data.text = train_data.text.apply(lambda x: scrub_words(x))\n",
    "test_data.text = test_data.text.apply(lambda x: scrub_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    # Tokenize\n",
    "    text = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Stemming\n",
    "    text = \" \".join([porter_stemmer.stem(word = word) for word in text])\n",
    "    \n",
    "    return text\n",
    "\n",
    "train_data.text = train_data.text.apply(lambda x: stem_text(x))\n",
    "test_data.text = test_data.text.apply(lambda x: stem_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spacy Stopwords\n",
    "# sp = spacy.load('en_core_web_sm')\n",
    "# spacy_stopwords = sp.Defaults.stop_words\n",
    "spacy_stopwords = ['few', 'third', 'other', 'among', 're', 'really', 'four', 'hence', 'whatever', 'already', 'quite', 'ours', 'even', 'than', 'about', 'mine', 'he', 'through', 'are', 'sometime', 'full', 'with', 'besides', 'becomes', 'rather', 'per', 'on', 'used', 'anyway', 'his', 'after', 'everywhere', 'whence', 'see', 'next', 'just', 'though', 'yet', 'nine', 'most', 'made', 'never', 'out', 'themselves', 'thereafter', 'eleven', 'moreover', 'please', 'its', 'fifty', 'himself', 'myself', 'so', 'an', 'keep', 'could', '‘d', 'using', 'wherever', 'around', 'once', 'in', 'unless', 'somehow', 'whereupon', 'cannot', 'namely', 'behind', 'to', 'last', 'however', 'me', 'beyond', 'of', 'were', 'being', 'put', 'or', 'because', 'another', 'herself', 'same', 'since', 'become', 'nowhere', 'they', 'my', 'whither', 'anyone', 'why', 'those', 'therefore', 'onto', 'doing', 'perhaps', 'there', 'i', 'amongst', 'sixty', 'while', 'up', 'although', 'get', 'she', 'these', 'latterly', 'no', 'nor', 'mostly', '’d', 'had', 'should', 'whenever', 'go', 'six', 'where', 'whom', 'became', 'again', 'until', 'under', 'yourselves', 'your', 'has', 'twenty', 'hereby', 'elsewhere', 'done', 'beside', 'you', 'not', 'below', 'along', 'make', 'but', 'someone', 'her', 'still', 'their', 'first', 'ever', 'forty', 'will', 'herein', '‘ve', 'would', 'many', 'thence', 'indeed', 'which', 'enough', 'nevertheless', 'towards', 'whereas', 'several', 'for', 'itself', 'noone', '’re', 'empty', 'our', 'ca', 'all', 'latter', '’s', 'such', 'thru', \"'ve\", 'and', 'been', 'upon', 'seemed', 'before', 'above', 'by', 'one', 'various', 'hundred', 'less', 'them', 'amount', 'neither', 'might', 'us', 'say', 'toward', 'some', 'hers', 'now', 'can', 'only', '’m', 'own', '‘re', 'be', 'least', 'thereupon', 'well', 'throughout', 'often', 'always', \"n't\", 'ten', 'am', 'ourselves', 'fifteen', 'is', 'formerly', 'name', 'then', 'alone', 'seems', 'whereby', 'nothing', 'across', 'via', '‘ll', '‘m', 'the', \"'m\", 'as', 'n‘t', 'we', 'call', 'except', 'every', 'whereafter', 'further', 'five', 'former', 'any', 'against', 'serious', 'without', 'hereafter', 'each', 'anywhere', \"'re\", 'do', 'did', 'else', 'down', 'almost', 'due', 'also', 'at', 'wherein', 'from', 'anyhow', 'afterwards', 'twelve', 'otherwise', \"'s\", 'too', 'give', 'something', '’ve', 'two', 'side', 'within', 'him', 'front', 'hereupon', 'here', 'thereby', 'may', 'this', 'seem', 'top', 'becoming', 'beforehand', 'either', \"'d\", 'during', 'a', 'when', 'off', 'thus', 'move', '’ll', 'whose', 'back', 'was', 'everything', 'whole', 'everyone', 'show', 'between', 'it', 'anything', 'that', 'bottom', 'part', 'over', 'somewhere', 'more', 'whoever', 'n’t', 'into', 'take', 'who', 'whether', 'does', 'seeming', 'both', 'none', 'much', 'yourself', 'meanwhile', 'regarding', 'must', '‘s', 'have', 'if', \"'ll\", 'therein', 'very', 'what', 'how', 'others', 'yours', 'nobody', 'eight', 'three', 'together', 'sometimes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word to TfidfVec\n",
    "tv = TfidfVectorizer(stop_words = spacy_stopwords, token_pattern = \"(?u)\\\\b\\\\w+\\\\b\", smooth_idf = True, max_features = 10000)\n",
    "x_train = tv.fit_transform(train_data.text).toarray()\n",
    "x_test = tv.fit_transform(test_data.text).toarray()\n",
    "y_train = train_data.label\n",
    "y_test = test_label.label.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def Evaluation(y_test, preds):\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, preds))\n",
    "    print(\"Accuracy: %.5g\" %(metrics.accuracy_score(y_test, preds)))\n",
    "    print(\"Precision: %.5g\" %(metrics.precision_score(y_test, preds, pos_label = '1')))\n",
    "    print(\"Recall: %.5g\" %(metrics.recall_score(y_test, preds, pos_label = '1')))\n",
    "    print(\"F1 Score: %.5g\" %(metrics.f1_score(y_test, preds, pos_label = '1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[614  16]\n",
      " [602  15]]\n",
      "Accuracy: 0.50441\n",
      "Precision: 0.48387\n",
      "Recall: 0.024311\n",
      "F1 Score: 0.046296\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "XGBoost_model = XGBClassifier(n_estimators = 100, max_features = 100, max_depth = 5, learning_rate = 0.1)\n",
    "XGBoost_model.fit(x_train, y_train)\n",
    "preds = XGBoost_model.predict(x_test)\n",
    "Evaluation(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[601  29]\n",
      " [595  22]]\n",
      "Accuracy: 0.4996\n",
      "Precision: 0.43137\n",
      "Recall: 0.035656\n",
      "F1 Score: 0.065868\n"
     ]
    }
   ],
   "source": [
    "# GBDT\n",
    "GBDT_model = GradientBoostingClassifier(n_estimators = 100, max_features = 100, max_depth = 5, learning_rate = 0.1)\n",
    "GBDT_model.fit(x_train, y_train)\n",
    "preds = GBDT_model.predict(x_test)\n",
    "Evaluation(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Confusion Matrix:\n",
      "[[601  29]\n",
      " [595  22]]\n",
      "Accuracy: 0.4996\n",
      "Precision: 0.43137\n",
      "Recall: 0.035656\n",
      "F1 Score: 0.065868\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "LightGBM_model = LGBMClassifier(n_estimators = 100, max_features = 100, max_depth = 5, learning_rate = 0.1)\n",
    "LightGBM_model.fit(x_train, y_train)\n",
    "prediction = LightGBM_model.predict(x_test)\n",
    "Evaluation(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
